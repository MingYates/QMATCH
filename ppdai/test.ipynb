{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T02:29:29.258195Z",
     "start_time": "2018-06-12T02:29:20.388610Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "questionsfile = '/files/faust/COMPETITION/ppdai/question.csv'\n",
    "questions  = {}\n",
    "with open(questionsfile) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        qtext  = {}\n",
    "        qtext['cwords'] = row['words'].strip().split(' ')\n",
    "        qtext['cchars'] = row['chars'].strip().split(' ')\n",
    "        questions[row['qid']] = qtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T02:29:43.098595Z",
     "start_time": "2018-06-12T02:29:43.094889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728428\n",
      "['W10139', 'W15218', 'W12566', 'W01490']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "iwords = questions['Q000012']['cwords']\n",
    "# iwords = questions['Q000012']['chars']\n",
    "print(iwords)\n",
    "print(type(iwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T02:30:01.469120Z",
     "start_time": "2018-06-12T02:29:57.420557Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/files/faust/COMPETITION/ppdai/questions.pkl', 'wb') as f:\n",
    "    pickle.dump(questions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramextend(wlist, n):\n",
    "    result = []\n",
    "    for i in range(len(wlist)):\n",
    "        for j in range(1, n+1):\n",
    "            if (i+j) <= len(wlist):\n",
    "                merged = ''.join(wlist[i:i+j])\n",
    "                result.append(merged)\n",
    "    return result\n",
    "    \n",
    "ws = ['W10139', 'W15218', 'W12566', 'W01490']\n",
    "result = ngramextend(ws, 3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_ngram = {}\n",
    "for q_id, wc in questions.items():\n",
    "    ori_w = wc['words']\n",
    "    ori_c = wc['chars']\n",
    "    qtext = {}\n",
    "    qtext['cwords'] = ngramextend(ori_w, 3)\n",
    "    qtext['cchars'] = ngramextend(ori_c, 5)\n",
    "    questions_ngram[q_id] = qtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/files/faust/COMPETITION/ppdai/questions_ngram_extend.pkl', 'wb') as f:\n",
    "    pickle.dump(questions_ngram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T02:30:25.525759Z",
     "start_time": "2018-06-12T02:30:19.813126Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# questions = pickle.load(open('/files/faust/COMPETITION/ppdai/questions_ngram_extend.pkl', 'rb'))\n",
    "questions = pickle.load(open('/files/faust/COMPETITION/ppdai/questions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T01:51:53.304776Z",
     "start_time": "2018-06-12T01:51:05.662910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus_words = [' '.join(wc['cwords']) for _, wc in questions.items()]\n",
    "vec_cw = TfidfVectorizer()\n",
    "vec_cw.fit(corpus_words)\n",
    "\n",
    "corpus_chars = [' '.join(wc['cchars']) for _, wc in questions.items()]\n",
    "vec_cc = TfidfVectorizer()\n",
    "vec_cc.fit(corpus_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T02:30:51.799628Z",
     "start_time": "2018-06-12T02:30:50.592382Z"
    }
   },
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "\n",
    "for _, wc in questions.items():\n",
    "    for word in wc['cwords']:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T02:31:18.318756Z",
     "start_time": "2018-06-12T02:31:18.311613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20890\n",
      "11202\n"
     ]
    }
   ],
   "source": [
    "print(len(word_count))\n",
    "word_count_filt = {}\n",
    "for w,f in word_count.items():\n",
    "    if f > 1:\n",
    "        word_count_filt[w] = f\n",
    "print(len(word_count_filt))\n",
    "wdict = [w for w,_ in word_count_filt.items()]\n",
    "wdict = dict((k,v) for v,k in enumerate(wdict))\n",
    "print(wdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T05:58:34.482169Z",
     "start_time": "2018-06-12T05:58:34.468654Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bag_of_words(sentence):\n",
    "    vector = np.zeros((len(wdict) + 1,), dtype=np.int32)\n",
    "    for w in sentence:\n",
    "        if w in wdict:\n",
    "            vector[wdict[w]] = 1\n",
    "        else:\n",
    "            vector[-1] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T06:01:45.414290Z",
     "start_time": "2018-06-12T06:01:45.213785Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "indim = len(wdict) + 1\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1024, input_dim=indim))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(units=1024))\n",
    "model.add(Activation(\"tanh\"))\n",
    "model.add(Dense(units=indim))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T03:28:50.737653Z",
     "start_time": "2018-06-12T03:28:49.982844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254386\n"
     ]
    }
   ],
   "source": [
    "def getvector_with_id(qid):\n",
    "    words = questions[qid]['cwords']\n",
    "    vector =bag_of_words(words)\n",
    "    return vector\n",
    "\n",
    "# read train pair\n",
    "data = []\n",
    "import csv\n",
    "trainfile = '/files/faust/COMPETITION/ppdai/train.csv'\n",
    "with open(trainfile) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append((row['q1'], row['q2'], row['label']))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T03:10:45.895812Z",
     "start_time": "2018-06-12T03:10:45.726537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228947\n",
      "25439\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(data)\n",
    "traindata = data[:int(len(data)*0.9)]\n",
    "testdata  = data[int(len(data)*0.9):]\n",
    "print(len(traindata))\n",
    "print(len(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-11T06:00:39.268Z"
    }
   },
   "outputs": [],
   "source": [
    "## for translation-based/alignment\n",
    "traindata_pos = [q for q in traindata if q[2] == '1']\n",
    "testdata_pos = [q for q in testdata if q[2] == '1']\n",
    "print(len(traindata_pos))\n",
    "\n",
    "qidlist = list(questions.keys())\n",
    "qid_sample = random.sample(qidlist, len(traindata_pos))\n",
    "traindata_copy = [(qid,qid,1) for qid in qid_sample]\n",
    "\n",
    "traindata_pos.extend(traindata_copy)\n",
    "random.shuffle(traindata_pos)\n",
    "print(len(traindata_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T05:58:55.234795Z",
     "start_time": "2018-06-12T05:58:48.750025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118835\n",
      "237670\n"
     ]
    }
   ],
   "source": [
    "train_x = np.array([getvector_id(q[0]) for q in traindata_copy])\n",
    "train_y = np.array([getvector_id(q[1]) for q in traindata_copy])\n",
    "test_x = np.array([getvector_id(q[0]) for q in testdata_pos])\n",
    "test_y = np.array([getvector_id(q[1]) for q in testdata_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T06:12:44.643111Z",
     "start_time": "2018-06-12T06:01:59.882904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  9600/118835 [=>............................] - ETA: 49s - loss: 4.9849e-04 - mean_squared_error: 4.9923e-0"
     ]
    }
   ],
   "source": [
    "EPOCH = 20\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.025\n",
    "model.fit(train_x, train_y, epochs=10, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
